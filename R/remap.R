#' Build separate models for mapping multiple regions.
#'
#' Separate models are built for each given region and combined into one S3
#' object that can be used to predict on new data using generic function
#' predict(). If a model fails for a region, a warning is given but the modeling
#' process will continue.
#'
#'
#' @param data An sf data frame with point geometry.
#' @param model_function A function that can take a subset of 'data' and
#' output a model that can be used to predict new values when passed to generic
#' function predict().
#' @param buffer A buffer zone around each region in km where data is included
#' in the data used to build models for each region. (Can be a named vector
#' with different values for each unique 'region_id' in 'region'.)
#' @param min_n The minimum number of observations to use when building a model.
#' If there are not enough observations in the region and buffer, then the
#' closest min_n observations are used. No minimum if set to 0.
#' @param regions An sf dataframe with polygon or multipolygon geometry.
#' @param region_id Optional name of column in 'regions' that contains the id
#' that each region belongs to (no quotes). If null, it will be assumed that
#' each polygon is its own region (no regions have more than one polygon).
#' @param distances An optional matrix of distances between 'data' and 'regions'
#' generated by redist() function (calculated internally if not
#' provided).
#' @param progress If true, a text progress bar is printed to the console.
#' @param keep_data If true, extra data is included in the output object. Extra
#' data is 'data', 'distances', and 'data_indices', (see output description).
#' @param ... Extra arguments to pass to 'model_function' function.
#'
#' @return A \emph{remap} S3 object containing:
#' \describe{
#'   \item{\emph{models}}{A list of models containing a model output by
#'   'model_function' for each region.}
#'   \item{\emph{regions}}{'regions' object passed to the function (used for
#'   prediction).}
#'   \item{\emph{region_id}}{'region_id' object passed to the function (used for
#'   prediction).}
#'   \item{\emph{call}}{Shows the parameters that were passed to the function.}
#'   \item{\emph{data}}{'data' object passed to the function
#'   ('keep_data' = TRUE).}
#'   \item{\emph{distances}}{Distances returned by internal call of
#'   redist() function ('keep_data' = TRUE).}
#'   \item{\emph{data_indices}}{A list of indices of rows of 'data' that were
#'   used to build each regional model ('keep_data' = TRUE).}
#' }
#'
#' @seealso
#'   \code{\link{predict.remap}} - used for predicting on new data.
#'   \code{\link{redist}} - used for building models within a buffer.
#'
#'
#' @export
remap <- function(data, model_function, buffer, min_n = 0,
                  regions, region_id, distances,
                  progress = FALSE, keep_data = FALSE, ...) {
  # Check input
  # ============================================================================
  check_input(data, regions, distances)

  # check if region_id is a character, if it is not, make it a character vector
  if (!missing(region_id) &&
      !tryCatch(is.character(region_id), error = function(e) FALSE)) {
    region_id <- deparse(substitute(region_id))
  }

  # process regions so only one line makes up a region
  regions <- process_regions(regions, region_id)
  region_id <- names(regions)[[1]]

  # Find distances between the data and each region
  # ============================================================================
  if (missing(distances)) {
    if(progress) cat("Finding distances...\n")
    distances <- redist(data, regions = regions, region_id = region_id,
                        progress = progress)
  }

  id_list <- colnames(distances)

  # Check buffer
  # ===========================================================================
  if (missing(buffer) || any(is.na(buffer)) ||
      !is.numeric(buffer) || any(buffer < 0)) {
    stop("'buffer' must be a number >= 0.")
  }
  if (length(buffer) == 1) {
    buffer <- rep(buffer, length(id_list))
    names(buffer) <- id_list
  } else if (!all(names(buffer) %in% id_list)) {
    stop("'buffer' values must have names equal to unique values",
         "in the 'region_id' column of 'regions'.")
  }
  buffer <- buffer[id_list]
  units(buffer) <- with(units::ud_units, km)

  # Reduce areas to places that have values present
  # ===========================================================================
  keep <-  apply(distances, 2, function(x) sum(x == 0 & !is.na(x)) > 0)
  id_list <- id_list[keep]
  buffer <- buffer[id_list]
  distances <- distances[, id_list]
  regions <- regions[regions[[region_id]] %in% id_list, ]

  # Find models for each region id
  # ============================================================================
  models <- list()
  data_indices <- list()

  # add progress bar
  if (progress) {
    cat("\nBuilding models...\n")
    pb <- utils::txtProgressBar(min = 0, max = length(id_list), style = 3)
    i <- 1
  }

  for (id in id_list) {
    # get indices for building model
    indices <- which(distances[, id] < buffer[[id]])
    if (length(indices) < min_n) {
      indices <- order(distances[, id])[1:min_n]
    }

    # try building a model and warn if one fails
    models[[id]] <- tryCatch({
      model_function(data[indices, ], ...)
    },
    error = function(e) {
      warning("Error in model for region ", id, ":\n", e)
      NULL
    })

    # Save data row indices used for building each model if keeping data
    if (keep_data) data_indices[[id]] <- indices

    # update progress bar
    if (progress) {
      utils::setTxtProgressBar(pb, i)
      i <- i + 1
    }
  }

  if (progress) cat("\n")

  # remove regions where model failed
  models <- models[sapply(models, function(x) !is.null(x))]
  id_list <- id_list[id_list %in% names(models)]
  regions <- regions[regions[[region_id]] %in% id_list, ]


  # Create output
  # ============================================================================
  output <- list(models = models, regions = regions, call = match.call())

  if (keep_data) {
    output[["data"]] <- data
    output[["distances"]] <- distances
    output[["data_indices"]] <- data_indices
  }

  class(output) <- "remap"
  return(output)
}





#` \emph{remap} prediction function.
#'
#' Make predictions given a set of data and smooths predictions between regions.
#' If an observation is outside of all regions and smoothing distances, the
#' closest region will be used to predict.
#'
#' @param object \emph{} S3 object output from sboost.
#' @param data An sf dataframe with point geometry.
#' @param smooth The distance in km within a region where a smooth transition
#' to the next region starts. (Can be a named vector with different values for
#' each unique object$region_id' in 'object$region'.)
#' @param distances An optional matrix of distances between 'data' and
#' 'object$regions' generated by redist() function (calculated
#' internally if not provided).
#' @param cores Number of cores for parallel computing. 'cores' above
#' default of 1 will require more memory.
#' @param progress If true, a text progress bar is printed to the console.
#' @param ... Arguments to pass to individual model prediction functions.
#'
#' @return Predictions in the form of a vector.
#'
#' @seealso \code{\link{remap}} building a regional model.
#'
#' @export
predict.remap <- function(object, data, smooth, distances, cores = 1,
                          progress = FALSE, ...) {
  # Check input
  # ============================================================================
  check_input(data, distances = distances)
  id_list <- names(object$models)

  # Check smooth
  # ===========================================================================
  if (missing(smooth) || any(is.na(smooth)) ||
      !is.numeric(smooth) || any(smooth <= 0)) {
    stop("'smooth' must be a number > 0.")
  } else if(length(smooth) == 1) {
    smooth <- rep(smooth, length(id_list))
    names(smooth) <- id_list
  } else if (!all(names(smooth) %in% id_list)) {
    stop("'smooth' values must have names equal to unique values",
         "in the 'object$region_id' column of 'object$regions'.")
  }
  smooth <- smooth[id_list]
  units(smooth) <- with(units::ud_units, km)

  # Find distances between the data and each region
  # ============================================================================
  if (missing(distances)) {
    if(progress) cat("Finding distances...\n")
    distances <- redist(data,
                        regions = object$regions[],
                        region_id = names(object$regions)[[1]],
                        progress = progress)
  }

  # make sure all values have a distance within 'smooth' of a region
  distances[t(apply(distances, 1, function(x) {
    x >= as.numeric(smooth) & x == min(x, na.rm = TRUE) & !is.na(x)
  }))] <- 0

  # Make predictions and smooth to final output
  # ============================================================================
  output <- rep(as.numeric(NA), nrow(data))
  weightsum <- rep(0, nrow(data))

  # add progress bar
  if (progress) {
    cat("\nPredicting...\n")
    pb <- utils::txtProgressBar(min = 0, max = length(id_list), style = 3)
    i <- 1
  }

  # do predictions in parallel if specified
  if (cores > 1) {
    clusters <- parallel::makeCluster(cores)

    parallel::clusterExport(clusters,
                            c("object", "data", "distances", "smooth",
                              unlist(lapply(search(), function(x) {
                                objects(x, pattern = "predict")
                              }))),
                            envir = environment())

    pred_list <- parallel::parLapply(
      clusters,
      as.list(id_list),
      function(id) {
        indices <- which(distances[, id] < smooth[[id]] &
                           !is.na(distances[, id]))

        stats::predict(object$models[[id]], data[indices, ])
      }
    )

    parallel::stopCluster(clusters)

    names(pred_list) <- id_list
  }

  # use running weighted average
  #   \mew_1 = x_1
  #   \mew_k = \mew_{k-1} + (w_k / \sum_1^k{w_i}) * (x_k - \mew_{k-1})
  for (id in id_list) {
    # only consider values within smoothing range
    indices <- distances[, id] < smooth[[id]] & !is.na(distances[, id])

    # make predictions
    if (cores > 1) {
      preds <- pred_list[[id]]
    } else {
      preds <- stats::predict(object$models[[id]], data[indices, ])
    }

    # update weights
    weight <- as.numeric(
      ((smooth[[id]] - distances[indices, id]) / smooth[[id]])^2
    )
    weightsum[indices] <- weightsum[indices] + weight

    # update output (k > 1)
    output[indices] <- output[indices] +
      (weight / weightsum[indices]) * (preds - output[indices])

    # update output (k == 1)
    starting <- indices & is.na(output)
    output[starting] <- preds[starting[indices]]

    # update progress bar
    if (progress) {
      utils::setTxtProgressBar(pb, i)
      i <- i + 1
    }
  }

  if (progress) cat("\n")

  return(output)
}

















#' Build separate models for mapping multiple regions.
#'
#' Separate models are built for each given region and combined into one S3
#' object that can be used to predict on new data using generic function
#' predict(). If a model fails for a region, a warning is given but the modeling
#' process will continue.
#'
#'
#' @param data An sf data frame with point geometry.
#' @param regions An sf dataframe with polygon or multipolygon geometry.
#' @param region_id Optional name of column in 'regions' that contains the id
#' that each region belongs to (no quotes). If null, it will be assumed that
#' each polygon is its own region (no regions have more than one polygon).
#' @param model_function A function that can take a subset of 'data' and
#' output a model that can be used to predict new values when passed to generic
#' function predict().
#' @param buffer A buffer zone around each region in km where data is included
#' in the data used to build models for each region. (Can be a named vector
#' with different values for each unique 'region_id' in 'region'.)
#' @param min_n The minimum number of observations to use when building a model.
#' If there are not enough observations in the region and buffer, then the
#' closest min_n observations are used. No minimum if set to 0.
#' @param distances An optional matrix of distances between 'data' and 'regions'
#' generated by redist() function (calculated internally if not
#' provided). Note that unless you know that you have min_n within a certain
#' distance, no max_dist parameter should be used in redist().
#' @param cores Number of cores for parallel computing. 'cores' above
#' default of 1 will require more memory.
#' @param progress If true, a text progress bar is printed to the console.
#' @param ... Extra arguments to pass to 'model_function' function.
#'
#' @return A \emph{remap} S3 object containing:
#' \describe{
#'   \item{\emph{models}}{A list of models containing a model output by
#'   'model_function' for each region.}
#'   \item{\emph{regions}}{'regions' object passed to the function (used for
#'   prediction).}
#'   \item{\emph{region_id}}{'region_id' object passed to the function (used for
#'   prediction).}
#'   \item{\emph{call}}{Shows the parameters that were passed to the function.}
#' }
#'
#' @seealso
#'   \code{\link{predict.remap}} - used for predicting on new data.
#'   \code{\link{redist}} - used for building models within a buffer.
#'
#'
#' @export
remap <- function(data, regions, region_id, model_function, buffer, min_n = 0,
                  distances, cores = 1, progress = FALSE,  ...) {
  # Check input
  # ============================================================================
  check_input(data = data,
              cores = cores,
              regions = regions,
              distances = distances)

  # check if region_id is a character, if it is not, make it a character
  if (!missing(region_id) &&
      !tryCatch(is.character(region_id), error = function(e) FALSE)) {
    region_id <- deparse(substitute(region_id))
  }

  # process regions so only one line makes up a region
  regions <- process_regions(regions, region_id)
  region_id <- names(regions)[[1]]
  id_list <- regions[[1]]

  # check numbers
  buffer <- process_numbers(x = buffer, name = "buffer", id_list = id_list)
  if (min_n < 0) stop("'min_n' needs to be a number >= 0.")

  # Find distances between the data and each region
  # ============================================================================
  if (missing(distances)) {
    distances <- redist(data,
                        regions = regions,
                        region_id = region_id,
                        cores = cores,
                        progress = progress)
  }

  # Reduce areas to places that have values present
  # ============================================================================
  keep <-  apply(distances, 2, function(x) sum(x == 0 & !is.na(x)) > 0)
  id_list <- id_list[keep]

  # Make function for building models
  # ============================================================================
  make_model <- function(id) {
    # get indices for building model
    indices <- which(distances[, id] < buffer[[id]])
    if (length(indices) < min_n) {
      indices <- order(distances[, id])[1:min_n]
    }

    # correct data if running in parallel
    if (cores > 1) {
      newdata <- parallel_hack(data[indices, ], sf::st_crs(data))
    } else {
      newdata <- data[indices, ]
    }

    # try building a model and warn if one fails
    tryCatch({
      model_function(newdata, ...)
    },
    error = function(e) {
      warning("Error in model for region ", id, ":\n", e)
      NULL
    })
  }

  # Find models for each region id (parallel)
  # ============================================================================
  if (progress) cat("Building models...\n")
  if (cores > 1) {

    clusters <- parallel::makeCluster(cores)

    parallel::clusterExport(clusters,
                            "model_function",
                            envir = environment())

    models <- parallel::parLapply(
      cl = clusters,
      X = as.list(id_list),
      fun = make_model
    )

    names(models) <- id_list

    parallel::stopCluster(clusters)

  # Find models for each region id (single core)
  # ============================================================================
  } else {
    models <- list()

    # add progress bar
    if (progress) {
      pb <- utils::txtProgressBar(min = 0, max = length(id_list), style = 3)
      i <- 1
    }

    for (id in id_list) {
      models[[id]] <- make_model(id)

      # update progress bar
      if (progress) {
        utils::setTxtProgressBar(pb, i)
        i <- i + 1
      }
    }

    if (progress) cat("\n")
  }

  # Create output
  # ============================================================================
  if (length(models) == 0) stop("Modeling function failed in every region.")

  # remove regions where model failed
  models <- models[sapply(models, function(x) !is.null(x))]
  id_list <- id_list[id_list %in% names(models)]
  regions <- regions[regions[[region_id]] %in% id_list, ]

  output <- list(models = models, regions = regions, call = match.call())

  class(output) <- "remap"
  return(output)
}





#` \emph{remap} prediction function.
#'
#' Make predictions given a set of data and smooths predictions between regions.
#' If an observation is outside of all regions and smoothing distances, the
#' closest region will be used to predict.
#'
#' @param object \emph{} S3 object output from sboost.
#' @param data An sf dataframe with point geometry.
#' @param smooth The distance in km within a region where a smooth transition
#' to the next region starts. (Can be a named vector with different values for
#' each unique object$region_id' in 'object$region'.)
#' @param distances An optional matrix of distances between 'data' and
#' 'object$regions' generated by redist() function (calculated
#' internally if not provided).
#' @param cores Number of cores for parallel computing. 'cores' above
#' default of 1 will require more memory.
#' @param progress If true, a text progress bar is printed to the console.
#' @param ... Arguments to pass to individual model prediction functions.
#'
#' @return Predictions in the form of a vector.
#'
#' @seealso \code{\link{remap}} building a regional model.
#'
#' @export
predict.remap <- function(object, data, smooth, distances, cores = 1,
                          progress = FALSE, ...) {
  # Check input
  # ============================================================================
  check_input(data, cores, distances = distances)
  id_list <- names(object$models)

  # check smooth
  smooth <- process_numbers(smooth, "smooth", id_list)

  # Find distances between the data and each region
  # ============================================================================
  if (missing(distances)) {
    distances <- redist(data,
                        regions = object$regions[],
                        region_id = names(object$regions)[[1]],
                        max_dist = smooth,
                        cores = cores,
                        progress = progress)
  }

  # make sure all values have a distance with non-zero weight
  distances[t(apply(distances, 1, function(x) {
    x >= as.numeric(smooth) & x == min(x, na.rm = TRUE) & !is.na(x)
  }))] <- 0

  # Do predictions in parallel if specified
  # ============================================================================
  if (progress) cat("Predicting...\n")
  if (cores > 1) {
    clusters <- parallel::makeCluster(cores)

    parallel::clusterExport(clusters,
                            c(unlist(lapply(search(), function(x) {
                                objects(x, pattern = "predict")
                              }))),
                            envir = environment())

    pred_list <- parallel::parLapply(
      clusters,
      as.list(id_list),
      function(id) {
        indices <- which(distances[, id] < smooth[[id]] &
                           !is.na(distances[, id]))

        # correct data to run in parallel
        newdata <- parallel_hack(data[indices, ], sf::st_crs(data))

        stats::predict(object$models[[id]], newdata, ...)
      }
    )

    parallel::stopCluster(clusters)

    names(pred_list) <- id_list
  }

  # Make predictions (if single core) and smooth to final output
  # ============================================================================
  output <- rep(as.numeric(NA), nrow(data))
  weightsum <- rep(0, nrow(data))

  if (progress) {
    pb <- utils::txtProgressBar(min = 0, max = length(id_list), style = 3)
    i <- 1
  }

  # use running weighted average
  #   \mew_1 = x_1
  #   \mew_k = \mew_{k-1} + (w_k / \sum_1^k{w_i}) * (x_k - \mew_{k-1})
  for (id in id_list) {
    # only consider values within smoothing range
    indices <- distances[, id] < smooth[[id]] & !is.na(distances[, id])

    # make predictions
    if (cores > 1) {
      preds <- pred_list[[id]]
    } else {
      preds <- stats::predict(object$models[[id]], data[indices, ], ...)
    }

    # update weights
    weight <- as.numeric(
      ((smooth[[id]] - distances[indices, id]) / smooth[[id]])^2
    )
    weightsum[indices] <- weightsum[indices] + weight

    # update output (k > 1)
    output[indices] <- output[indices] +
      (weight / weightsum[indices]) * (preds - output[indices])

    # update output (k == 1)
    starting <- indices & is.na(output)
    output[starting] <- preds[starting[indices]]

    # update progress bar
    if (progress) {
      utils::setTxtProgressBar(pb, i)
      i <- i + 1
    }
  }

  if (progress) cat("\n")

  return(output)
}



#' Print method for remap object.
#' @export
print.remap <- function(x, ...) {
  cat(paste("remap model with",
            length(x$models),
            "regional models"))
}



#' Plot method for remap object.
#'
#' Plots the regions used for modeling.
#'
#' @export
plot.remap <- function(x, ...) {
  plot(x$regions)
}



# parallel_hack
# ==============================================================================
# This is a hack for a bug that only comes up occasionally when running
# parallel code. For some reason when remap is run in parallel, the geometry
# column of the subset data object gets stripped of its class and returns
# to a "list" object. The workaround is to convert the geometry list to
# an sfc object and reassign the crs.
# Input:
#   data_ss - subset of data points
#   crs - the correct crs returned by sf::st_crs
# Output:
#   A corrected sf object
# ==============================================================================
parallel_hack <- function(data_ss, crs) {
  geom_col <- attr(data_ss, "sf_column")
  data_ss[[geom_col]] <- sf::st_sfc(data_ss[[geom_col]])
  sf::st_crs(data_ss) <- crs
  return(data_ss)
}






